# Phase 1 Ranking Runs: GLU Expansion Sweep
# Goal: Run to 5M tokens and identify top-1 GLU expansion factor
# Kill rule: Stop if dev PPL â‰¥0.5 worse than baseline for 1M tokens

defaults:
  - base_config

model:
  total_params: 10_000_000
  vocab_size: 16_000
  embedding_ratio: 0.35  # Use baseline for Phase 1 (can update after emb sweep)

experiments:
  - name: phase1_glu2x
    description: "Phase 1: 2.0x GLU expansion"
    model:
      glu_expansion: 2.0
    training:
      output_dir: "./outputs/phase1/glu2x"
      checkpoint_dir: "./checkpoints/phase1/glu2x"
      run_name: "phase1_glu2x"
      total_tokens: 5_000_000  # Phase 1: 5M tokens
      max_steps: -1
      eval_steps: 500
      save_steps: 2500
      logging_steps: 100
      early_stopping_enabled: true
      early_stopping_patience: 5
      early_stopping_threshold: 0.5
      early_stopping_min_tokens: 1_000_000

  - name: phase1_glu266x
    description: "Phase 1: 2.66x GLU expansion (SLIM target)"
    model:
      glu_expansion: 2.66
    training:
      output_dir: "./outputs/phase1/glu266x"
      checkpoint_dir: "./checkpoints/phase1/glu266x"
      run_name: "phase1_glu266x"
      total_tokens: 5_000_000
      max_steps: -1
      eval_steps: 500
      save_steps: 2500
      logging_steps: 100
      early_stopping_enabled: true
      early_stopping_patience: 5
      early_stopping_threshold: 0.5
      early_stopping_min_tokens: 1_000_000

  - name: phase1_glu3x
    description: "Phase 1: 3.0x GLU expansion"
    model:
      glu_expansion: 3.0
    training:
      output_dir: "./outputs/phase1/glu3x"
      checkpoint_dir: "./checkpoints/phase1/glu3x"
      run_name: "phase1_glu3x"
      total_tokens: 5_000_000
      max_steps: -1
      eval_steps: 500
      save_steps: 2500
      logging_steps: 100
      early_stopping_enabled: true
      early_stopping_patience: 5
      early_stopping_threshold: 0.5
      early_stopping_min_tokens: 1_000_000

  - name: phase1_glu4x
    description: "Phase 1: 4.0x GLU expansion (standard)"
    model:
      glu_expansion: 4.0
    training:
      output_dir: "./outputs/phase1/glu4x"
      checkpoint_dir: "./checkpoints/phase1/glu4x"
      run_name: "phase1_glu4x"
      total_tokens: 5_000_000
      max_steps: -1
      eval_steps: 500
      save_steps: 2500
      logging_steps: 100
      early_stopping_enabled: true
      early_stopping_patience: 5
      early_stopping_threshold: 0.5
      early_stopping_min_tokens: 1_000_000

# Shared settings for all Phase 1 experiments
training:
  batch_size: 32  # Reduced for 22GB GPU with parallel execution
  gradient_accumulation_steps: 8  # Effective batch size = 32 * 8 = 256
  learning_rate: 3e-4
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  max_grad_norm: 1.0

  # Schedule
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.05

  # Mixed precision
  fp16: false
  bf16: false
  tf32: true

  # Reproducibility
  seed: 42

evaluation:
  eval_batch_size: 512
  metrics:
    - "perplexity"
    - "loss"
  # Skip expensive evaluation in Phase 1
  skip_smallbench: true
  skip_latency_profiling: true
  skip_throughput_profiling: true

logging:
  use_wandb: false
  use_tensorboard: true
  log_level: "INFO"

infrastructure:
  device: "cuda"
  n_gpu: 1

paths:
  data_dir: "./data/lm_tokenized"
