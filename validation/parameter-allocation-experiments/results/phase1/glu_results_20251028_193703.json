[
  {
    "exp_name": "phase1_glu3x",
    "status": "failed",
    "elapsed_seconds": 5.111790657043457,
    "error": "Command '['/home/ubuntu/stellux/validation/parameter-allocation-experiments/venv/bin/python', 'run_experiment.py', '--config', 'configs/phase1_glu_sweep.yaml', '--exp', 'phase1_glu3x']' returned non-zero exit status 1.",
    "stdout": "                0       0.00%\n  (tied={config.tied_lm_head})\n--------------------------------------------------------------------------------\nTransformer Backbone:                6,153,840      63.31%\n  Attention (per layer)                186,624\n  FFN/SwiGLU (per layer)               371,952\n  LayerNorm (per layer)                    864\n  Num layers                                11\nFinal LayerNorm                            432       0.00%\n--------------------------------------------------------------------------------\nTOTAL                                9,720,864       100.0%\n================================================================================\n\nModel Configuration:\n  d_model: 216\n  d_ff: 574\n  n_layers: 11\n  n_heads: 8\n  vocab_size: 16000\n  embedding_ratio: 35.00%\n  glu_expansion: 2.66x\n  tied_lm_head: True\n\nTarget params: 10,000,000\nActual params: 9,720,864\nDifference: -279,136 (-2.79%)\n================================================================================\n",
    "stderr": "ain - INFO - ================================================================================\n\nEpoch 1/1:   0%|          | 0/1148 [00:00<?, ?it/s]\nEpoch 1/1:   0%|          | 1/1148 [00:00<16:43,  1.14it/s]\nEpoch 1/1:   0%|          | 1/1148 [00:01<21:54,  1.15s/it]\n2025-10-28 19:37:02,446 - __main__ - ERROR - Experiment failed: CUDA out of memory. Tried to allocate 176.00 MiB. GPU 0 has a total capacity of 22.07 GiB of which 120.50 MiB is free. Process 7676 has 4.73 GiB memory in use. Including non-PyTorch memory, this process has 4.73 GiB memory in use. Process 7678 has 4.73 GiB memory in use. Process 7675 has 7.73 GiB memory in use. Of the allocated memory 3.76 GiB is allocated by PyTorch, and 709.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
  },
  {
    "exp_name": "phase1_glu266x",
    "status": "failed",
    "elapsed_seconds": 5.192404508590698,
    "error": "Command '['/home/ubuntu/stellux/validation/parameter-allocation-experiments/venv/bin/python', 'run_experiment.py', '--config', 'configs/phase1_glu_sweep.yaml', '--exp', 'phase1_glu266x']' returned non-zero exit status 1.",
    "stdout": "                0       0.00%\n  (tied={config.tied_lm_head})\n--------------------------------------------------------------------------------\nTransformer Backbone:                6,153,840      63.31%\n  Attention (per layer)                186,624\n  FFN/SwiGLU (per layer)               371,952\n  LayerNorm (per layer)                    864\n  Num layers                                11\nFinal LayerNorm                            432       0.00%\n--------------------------------------------------------------------------------\nTOTAL                                9,720,864       100.0%\n================================================================================\n\nModel Configuration:\n  d_model: 216\n  d_ff: 574\n  n_layers: 11\n  n_heads: 8\n  vocab_size: 16000\n  embedding_ratio: 35.00%\n  glu_expansion: 2.66x\n  tied_lm_head: True\n\nTarget params: 10,000,000\nActual params: 9,720,864\nDifference: -279,136 (-2.79%)\n================================================================================\n",
    "stderr": "ain - INFO - ================================================================================\n\nEpoch 1/1:   0%|          | 0/1148 [00:00<?, ?it/s]\nEpoch 1/1:   0%|          | 1/1148 [00:00<16:37,  1.15it/s]\nEpoch 1/1:   0%|          | 1/1148 [00:01<21:52,  1.14s/it]\n2025-10-28 19:37:02,452 - __main__ - ERROR - Experiment failed: CUDA out of memory. Tried to allocate 176.00 MiB. GPU 0 has a total capacity of 22.07 GiB of which 120.50 MiB is free. Including non-PyTorch memory, this process has 4.73 GiB memory in use. Process 7677 has 4.73 GiB memory in use. Process 7678 has 4.73 GiB memory in use. Process 7675 has 7.73 GiB memory in use. Of the allocated memory 3.76 GiB is allocated by PyTorch, and 709.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
  },
  {
    "exp_name": "phase1_glu4x",
    "status": "failed",
    "elapsed_seconds": 5.193052530288696,
    "error": "Command '['/home/ubuntu/stellux/validation/parameter-allocation-experiments/venv/bin/python', 'run_experiment.py', '--config', 'configs/phase1_glu_sweep.yaml', '--exp', 'phase1_glu4x']' returned non-zero exit status 1.",
    "stdout": "                0       0.00%\n  (tied={config.tied_lm_head})\n--------------------------------------------------------------------------------\nTransformer Backbone:                6,153,840      63.31%\n  Attention (per layer)                186,624\n  FFN/SwiGLU (per layer)               371,952\n  LayerNorm (per layer)                    864\n  Num layers                                11\nFinal LayerNorm                            432       0.00%\n--------------------------------------------------------------------------------\nTOTAL                                9,720,864       100.0%\n================================================================================\n\nModel Configuration:\n  d_model: 216\n  d_ff: 574\n  n_layers: 11\n  n_heads: 8\n  vocab_size: 16000\n  embedding_ratio: 35.00%\n  glu_expansion: 2.66x\n  tied_lm_head: True\n\nTarget params: 10,000,000\nActual params: 9,720,864\nDifference: -279,136 (-2.79%)\n================================================================================\n",
    "stderr": "ain - INFO - ================================================================================\n\nEpoch 1/1:   0%|          | 0/1148 [00:00<?, ?it/s]\nEpoch 1/1:   0%|          | 1/1148 [00:00<16:06,  1.19it/s]\nEpoch 1/1:   0%|          | 1/1148 [00:01<21:23,  1.12s/it]\n2025-10-28 19:37:02,454 - __main__ - ERROR - Experiment failed: CUDA out of memory. Tried to allocate 176.00 MiB. GPU 0 has a total capacity of 22.07 GiB of which 120.50 MiB is free. Process 7676 has 4.73 GiB memory in use. Process 7677 has 4.73 GiB memory in use. Including non-PyTorch memory, this process has 4.73 GiB memory in use. Process 7675 has 7.73 GiB memory in use. Of the allocated memory 3.76 GiB is allocated by PyTorch, and 709.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
  },
  {
    "exp_name": "phase1_glu2x",
    "status": "failed",
    "elapsed_seconds": 5.244964361190796,
    "error": "Command '['/home/ubuntu/stellux/validation/parameter-allocation-experiments/venv/bin/python', 'run_experiment.py', '--config', 'configs/phase1_glu_sweep.yaml', '--exp', 'phase1_glu2x']' returned non-zero exit status 1.",
    "stdout": "                0       0.00%\n  (tied={config.tied_lm_head})\n--------------------------------------------------------------------------------\nTransformer Backbone:                6,153,840      63.31%\n  Attention (per layer)                186,624\n  FFN/SwiGLU (per layer)               371,952\n  LayerNorm (per layer)                    864\n  Num layers                                11\nFinal LayerNorm                            432       0.00%\n--------------------------------------------------------------------------------\nTOTAL                                9,720,864       100.0%\n================================================================================\n\nModel Configuration:\n  d_model: 216\n  d_ff: 574\n  n_layers: 11\n  n_heads: 8\n  vocab_size: 16000\n  embedding_ratio: 35.00%\n  glu_expansion: 2.66x\n  tied_lm_head: True\n\nTarget params: 10,000,000\nActual params: 9,720,864\nDifference: -279,136 (-2.79%)\n================================================================================\n",
    "stderr": "ain - INFO - ================================================================================\n\nEpoch 1/1:   0%|          | 0/1148 [00:00<?, ?it/s]\nEpoch 1/1:   0%|          | 1/1148 [00:00<16:21,  1.17it/s]\nEpoch 1/1:   0%|          | 1/1148 [00:01<21:59,  1.15s/it]\n2025-10-28 19:37:02,454 - __main__ - ERROR - Experiment failed: CUDA out of memory. Tried to allocate 828.00 MiB. GPU 0 has a total capacity of 22.07 GiB of which 120.50 MiB is free. Process 7676 has 4.73 GiB memory in use. Process 7677 has 4.73 GiB memory in use. Process 7678 has 4.73 GiB memory in use. Including non-PyTorch memory, this process has 7.73 GiB memory in use. Of the allocated memory 6.68 GiB is allocated by PyTorch, and 796.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
  }
]