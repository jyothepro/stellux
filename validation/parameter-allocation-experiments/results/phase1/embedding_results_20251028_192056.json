[
  {
    "exp_name": "phase1_emb25",
    "status": "success",
    "elapsed_seconds": 237.56061029434204,
    "stdout": "                0       0.00%\n  (tied={config.tied_lm_head})\n--------------------------------------------------------------------------------\nTransformer Backbone:                6,153,840      63.31%\n  Attention (per layer)                186,624\n  FFN/SwiGLU (per layer)               371,952\n  LayerNorm (per layer)                    864\n  Num layers                                11\nFinal LayerNorm                            432       0.00%\n--------------------------------------------------------------------------------\nTOTAL                                9,720,864       100.0%\n================================================================================\n\nModel Configuration:\n  d_model: 216\n  d_ff: 574\n  n_layers: 11\n  n_heads: 8\n  vocab_size: 16000\n  embedding_ratio: 35.00%\n  glu_expansion: 2.66x\n  tied_lm_head: True\n\nTarget params: 10,000,000\nActual params: 9,720,864\nDifference: -279,136 (-2.79%)\n================================================================================\n",
    "stderr": "05<00:01, 16.96it/s]\nEvaluating:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 93/118 [00:05<00:01, 16.23it/s]\nEvaluating:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 95/118 [00:05<00:01, 16.84it/s]\nEvaluating:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 98/118 [00:05<00:01, 18.92it/s]\nEvaluating:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 101/118 [00:05<00:00, 18.23it/s]\nEvaluating:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 103/118 [00:05<00:00, 15.51it/s]\nEvaluating:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 106/118 [00:05<00:00, 16.46it/s]\nEvaluating:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 108/118 [00:06<00:00, 16.55it/s]\nEvaluating:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 111/118 [00:06<00:00, 17.71it/s]\nEvaluating:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 113/118 [00:06<00:00, 15.70it/s]\nEvaluating:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 115/118 [00:06<00:00, 15.18it/s]\nEvaluating:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 117/118 [00:06<00:00, 14.49it/s]\n                                                             \n2025-10-28 19:12:56,972 - train - INFO - Final metrics: {'eval_loss': 0.052549515070116265, 'eval_perplexity': 1.0539547474297644}\n2025-10-28 19:12:57,121 - utils.checkpointing - INFO - Checkpoint saved to checkpoints/checkpoint_final.pt\n"
  },
  {
    "exp_name": "phase1_emb35",
    "status": "success",
    "elapsed_seconds": 239.13337326049805,
    "stdout": "                0       0.00%\n  (tied={config.tied_lm_head})\n--------------------------------------------------------------------------------\nTransformer Backbone:                6,153,840      63.31%\n  Attention (per layer)                186,624\n  FFN/SwiGLU (per layer)               371,952\n  LayerNorm (per layer)                    864\n  Num layers                                11\nFinal LayerNorm                            432       0.00%\n--------------------------------------------------------------------------------\nTOTAL                                9,720,864       100.0%\n================================================================================\n\nModel Configuration:\n  d_model: 216\n  d_ff: 574\n  n_layers: 11\n  n_heads: 8\n  vocab_size: 16000\n  embedding_ratio: 35.00%\n  glu_expansion: 2.66x\n  tied_lm_head: True\n\nTarget params: 10,000,000\nActual params: 9,720,864\nDifference: -279,136 (-2.79%)\n================================================================================\n",
    "stderr": ":05<00:01, 16.99it/s]\nEvaluating:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 93/118 [00:05<00:01, 16.21it/s]\nEvaluating:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 95/118 [00:05<00:01, 16.70it/s]\nEvaluating:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 98/118 [00:05<00:01, 18.80it/s]\nEvaluating:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 101/118 [00:05<00:00, 18.03it/s]\nEvaluating:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 103/118 [00:05<00:00, 15.38it/s]\nEvaluating:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 106/118 [00:05<00:00, 16.30it/s]\nEvaluating:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 108/118 [00:06<00:00, 16.36it/s]\nEvaluating:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 111/118 [00:06<00:00, 17.63it/s]\nEvaluating:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 113/118 [00:06<00:00, 15.71it/s]\nEvaluating:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 115/118 [00:06<00:00, 15.13it/s]\nEvaluating:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 117/118 [00:06<00:00, 14.44it/s]\n                                                             \n2025-10-28 19:16:56,135 - train - INFO - Final metrics: {'eval_loss': 0.05273510657567927, 'eval_perplexity': 1.0541503706305757}\n2025-10-28 19:16:56,294 - utils.checkpointing - INFO - Checkpoint saved to checkpoints/checkpoint_final.pt\n"
  },
  {
    "exp_name": "phase1_emb45",
    "status": "success",
    "elapsed_seconds": 239.5720570087433,
    "stdout": "                0       0.00%\n  (tied={config.tied_lm_head})\n--------------------------------------------------------------------------------\nTransformer Backbone:                6,153,840      63.31%\n  Attention (per layer)                186,624\n  FFN/SwiGLU (per layer)               371,952\n  LayerNorm (per layer)                    864\n  Num layers                                11\nFinal LayerNorm                            432       0.00%\n--------------------------------------------------------------------------------\nTOTAL                                9,720,864       100.0%\n================================================================================\n\nModel Configuration:\n  d_model: 216\n  d_ff: 574\n  n_layers: 11\n  n_heads: 8\n  vocab_size: 16000\n  embedding_ratio: 35.00%\n  glu_expansion: 2.66x\n  tied_lm_head: True\n\nTarget params: 10,000,000\nActual params: 9,720,864\nDifference: -279,136 (-2.79%)\n================================================================================\n",
    "stderr": ":04<00:01, 16.86it/s]\nEvaluating:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 93/118 [00:05<00:01, 16.21it/s]\nEvaluating:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 95/118 [00:05<00:01, 16.83it/s]\nEvaluating:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 98/118 [00:05<00:01, 18.99it/s]\nEvaluating:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 101/118 [00:05<00:00, 18.15it/s]\nEvaluating:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 103/118 [00:05<00:00, 15.42it/s]\nEvaluating:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 106/118 [00:05<00:00, 16.33it/s]\nEvaluating:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 108/118 [00:06<00:00, 16.40it/s]\nEvaluating:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 111/118 [00:06<00:00, 17.61it/s]\nEvaluating:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 113/118 [00:06<00:00, 15.68it/s]\nEvaluating:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 115/118 [00:06<00:00, 15.15it/s]\nEvaluating:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 117/118 [00:06<00:00, 14.47it/s]\n                                                             \n2025-10-28 19:20:55,744 - train - INFO - Final metrics: {'eval_loss': 0.056202301755547523, 'eval_perplexity': 1.057811659247775}\n2025-10-28 19:20:55,894 - utils.checkpointing - INFO - Checkpoint saved to checkpoints/checkpoint_final.pt\n"
  }
]