[
  {
    "exp_name": "phase1_emb25",
    "status": "success",
    "elapsed_seconds": 236.74374437332153,
    "stdout": "                0       0.00%\n  (tied={config.tied_lm_head})\n--------------------------------------------------------------------------------\nTransformer Backbone:                6,153,840      63.31%\n  Attention (per layer)                186,624\n  FFN/SwiGLU (per layer)               371,952\n  LayerNorm (per layer)                    864\n  Num layers                                11\nFinal LayerNorm                            432       0.00%\n--------------------------------------------------------------------------------\nTOTAL                                9,720,864       100.0%\n================================================================================\n\nModel Configuration:\n  d_model: 216\n  d_ff: 574\n  n_layers: 11\n  n_heads: 8\n  vocab_size: 16000\n  embedding_ratio: 35.00%\n  glu_expansion: 2.66x\n  tied_lm_head: True\n\nTarget params: 10,000,000\nActual params: 9,720,864\nDifference: -279,136 (-2.79%)\n================================================================================\n",
    "stderr": ":05<00:01, 15.77it/s]\nEvaluating:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 94/118 [00:05<00:01, 15.45it/s]\nEvaluating:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 96/118 [00:05<00:01, 16.17it/s]\nEvaluating:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 99/118 [00:05<00:01, 18.31it/s]\nEvaluating:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 101/118 [00:05<00:00, 17.21it/s]\nEvaluating:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 103/118 [00:05<00:01, 14.48it/s]\nEvaluating:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 106/118 [00:06<00:00, 15.42it/s]\nEvaluating:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 108/118 [00:06<00:00, 15.54it/s]\nEvaluating:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 111/118 [00:06<00:00, 16.80it/s]\nEvaluating:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 113/118 [00:06<00:00, 15.16it/s]\nEvaluating:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 115/118 [00:06<00:00, 14.78it/s]\nEvaluating:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 117/118 [00:06<00:00, 14.23it/s]\n                                                             \n2025-10-28 20:14:53,061 - train - INFO - Final metrics: {'eval_loss': 0.05557772934119752, 'eval_perplexity': 1.0571511855441293}\n2025-10-28 20:14:53,272 - utils.checkpointing - INFO - Checkpoint saved to checkpoints/checkpoint_final.pt\n"
  },
  {
    "exp_name": "phase1_emb35",
    "status": "success",
    "elapsed_seconds": 242.5054168701172,
    "stdout": "                0       0.00%\n  (tied={config.tied_lm_head})\n--------------------------------------------------------------------------------\nTransformer Backbone:                6,153,840      63.31%\n  Attention (per layer)                186,624\n  FFN/SwiGLU (per layer)               371,952\n  LayerNorm (per layer)                    864\n  Num layers                                11\nFinal LayerNorm                            432       0.00%\n--------------------------------------------------------------------------------\nTOTAL                                9,720,864       100.0%\n================================================================================\n\nModel Configuration:\n  d_model: 216\n  d_ff: 574\n  n_layers: 11\n  n_heads: 8\n  vocab_size: 16000\n  embedding_ratio: 35.00%\n  glu_expansion: 2.66x\n  tied_lm_head: True\n\nTarget params: 10,000,000\nActual params: 9,720,864\nDifference: -279,136 (-2.79%)\n================================================================================\n",
    "stderr": "05<00:01, 16.92it/s]\nEvaluating:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 93/118 [00:05<00:01, 16.19it/s]\nEvaluating:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 95/118 [00:05<00:01, 16.79it/s]\nEvaluating:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 98/118 [00:05<00:01, 18.87it/s]\nEvaluating:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 101/118 [00:05<00:00, 18.16it/s]\nEvaluating:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 103/118 [00:05<00:00, 15.23it/s]\nEvaluating:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 106/118 [00:05<00:00, 16.11it/s]\nEvaluating:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 108/118 [00:06<00:00, 16.17it/s]\nEvaluating:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 111/118 [00:06<00:00, 17.44it/s]\nEvaluating:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 113/118 [00:06<00:00, 15.56it/s]\nEvaluating:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 115/118 [00:06<00:00, 15.04it/s]\nEvaluating:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 117/118 [00:06<00:00, 14.34it/s]\n                                                             \n2025-10-28 20:18:56,174 - train - INFO - Final metrics: {'eval_loss': 0.052984687384772806, 'eval_perplexity': 1.0544134991675347}\n2025-10-28 20:18:56,327 - utils.checkpointing - INFO - Checkpoint saved to checkpoints/checkpoint_final.pt\n"
  },
  {
    "exp_name": "phase1_emb45",
    "status": "success",
    "elapsed_seconds": 239.62705540657043,
    "stdout": "                0       0.00%\n  (tied={config.tied_lm_head})\n--------------------------------------------------------------------------------\nTransformer Backbone:                6,153,840      63.31%\n  Attention (per layer)                186,624\n  FFN/SwiGLU (per layer)               371,952\n  LayerNorm (per layer)                    864\n  Num layers                                11\nFinal LayerNorm                            432       0.00%\n--------------------------------------------------------------------------------\nTOTAL                                9,720,864       100.0%\n================================================================================\n\nModel Configuration:\n  d_model: 216\n  d_ff: 574\n  n_layers: 11\n  n_heads: 8\n  vocab_size: 16000\n  embedding_ratio: 35.00%\n  glu_expansion: 2.66x\n  tied_lm_head: True\n\nTarget params: 10,000,000\nActual params: 9,720,864\nDifference: -279,136 (-2.79%)\n================================================================================\n",
    "stderr": ":05<00:01, 16.91it/s]\nEvaluating:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 93/118 [00:05<00:01, 16.16it/s]\nEvaluating:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 95/118 [00:05<00:01, 16.78it/s]\nEvaluating:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 98/118 [00:05<00:01, 18.83it/s]\nEvaluating:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 101/118 [00:05<00:00, 18.14it/s]\nEvaluating:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 103/118 [00:05<00:00, 15.35it/s]\nEvaluating:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 106/118 [00:05<00:00, 16.25it/s]\nEvaluating:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 108/118 [00:06<00:00, 16.33it/s]\nEvaluating:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 111/118 [00:06<00:00, 17.50it/s]\nEvaluating:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 113/118 [00:06<00:00, 15.63it/s]\nEvaluating:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 115/118 [00:06<00:00, 15.12it/s]\nEvaluating:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 117/118 [00:06<00:00, 14.45it/s]\n                                                             \n2025-10-28 20:22:55,809 - train - INFO - Final metrics: {'eval_loss': 0.05310187777306171, 'eval_perplexity': 1.0545370735356419}\n2025-10-28 20:22:55,986 - utils.checkpointing - INFO - Checkpoint saved to checkpoints/checkpoint_final.pt\n"
  },
  {
    "exp_name": "phase1_glu2x",
    "status": "success",
    "elapsed_seconds": 239.3625020980835,
    "stdout": "                0       0.00%\n  (tied={config.tied_lm_head})\n--------------------------------------------------------------------------------\nTransformer Backbone:                6,153,840      63.31%\n  Attention (per layer)                186,624\n  FFN/SwiGLU (per layer)               371,952\n  LayerNorm (per layer)                    864\n  Num layers                                11\nFinal LayerNorm                            432       0.00%\n--------------------------------------------------------------------------------\nTOTAL                                9,720,864       100.0%\n================================================================================\n\nModel Configuration:\n  d_model: 216\n  d_ff: 574\n  n_layers: 11\n  n_heads: 8\n  vocab_size: 16000\n  embedding_ratio: 35.00%\n  glu_expansion: 2.66x\n  tied_lm_head: True\n\nTarget params: 10,000,000\nActual params: 9,720,864\nDifference: -279,136 (-2.79%)\n================================================================================\n",
    "stderr": ":05<00:01, 16.88it/s]\nEvaluating:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 93/118 [00:05<00:01, 16.18it/s]\nEvaluating:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 95/118 [00:05<00:01, 16.79it/s]\nEvaluating:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 98/118 [00:05<00:01, 18.85it/s]\nEvaluating:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 101/118 [00:05<00:00, 18.15it/s]\nEvaluating:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 103/118 [00:05<00:00, 15.38it/s]\nEvaluating:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 106/118 [00:05<00:00, 16.30it/s]\nEvaluating:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 108/118 [00:06<00:00, 16.36it/s]\nEvaluating:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 111/118 [00:06<00:00, 17.56it/s]\nEvaluating:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 113/118 [00:06<00:00, 15.64it/s]\nEvaluating:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 115/118 [00:06<00:00, 15.07it/s]\nEvaluating:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 117/118 [00:06<00:00, 14.40it/s]\n                                                             \n2025-10-28 20:26:55,192 - train - INFO - Final metrics: {'eval_loss': 0.05378011934300687, 'eval_perplexity': 1.0552525470204648}\n2025-10-28 20:26:55,341 - utils.checkpointing - INFO - Checkpoint saved to checkpoints/checkpoint_final.pt\n"
  },
  {
    "exp_name": "phase1_glu266x",
    "status": "success",
    "elapsed_seconds": 239.63458061218262,
    "stdout": "                0       0.00%\n  (tied={config.tied_lm_head})\n--------------------------------------------------------------------------------\nTransformer Backbone:                6,153,840      63.31%\n  Attention (per layer)                186,624\n  FFN/SwiGLU (per layer)               371,952\n  LayerNorm (per layer)                    864\n  Num layers                                11\nFinal LayerNorm                            432       0.00%\n--------------------------------------------------------------------------------\nTOTAL                                9,720,864       100.0%\n================================================================================\n\nModel Configuration:\n  d_model: 216\n  d_ff: 574\n  n_layers: 11\n  n_heads: 8\n  vocab_size: 16000\n  embedding_ratio: 35.00%\n  glu_expansion: 2.66x\n  tied_lm_head: True\n\nTarget params: 10,000,000\nActual params: 9,720,864\nDifference: -279,136 (-2.79%)\n================================================================================\n",
    "stderr": "05<00:01, 16.69it/s]\nEvaluating:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 93/118 [00:05<00:01, 16.06it/s]\nEvaluating:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 95/118 [00:05<00:01, 16.69it/s]\nEvaluating:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 98/118 [00:05<00:01, 18.80it/s]\nEvaluating:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 101/118 [00:05<00:00, 18.10it/s]\nEvaluating:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 103/118 [00:05<00:00, 15.32it/s]\nEvaluating:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 106/118 [00:05<00:00, 16.27it/s]\nEvaluating:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 108/118 [00:06<00:00, 16.34it/s]\nEvaluating:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 111/118 [00:06<00:00, 17.55it/s]\nEvaluating:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 113/118 [00:06<00:00, 15.66it/s]\nEvaluating:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 115/118 [00:06<00:00, 15.10it/s]\nEvaluating:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 117/118 [00:06<00:00, 14.41it/s]\n                                                             \n2025-10-28 20:30:54,816 - train - INFO - Final metrics: {'eval_loss': 0.055230165899116945, 'eval_perplexity': 1.0567838222839998}\n2025-10-28 20:30:54,987 - utils.checkpointing - INFO - Checkpoint saved to checkpoints/checkpoint_final.pt\n"
  },
  {
    "exp_name": "phase1_glu3x",
    "status": "success",
    "elapsed_seconds": 239.71129441261292,
    "stdout": "                0       0.00%\n  (tied={config.tied_lm_head})\n--------------------------------------------------------------------------------\nTransformer Backbone:                6,153,840      63.31%\n  Attention (per layer)                186,624\n  FFN/SwiGLU (per layer)               371,952\n  LayerNorm (per layer)                    864\n  Num layers                                11\nFinal LayerNorm                            432       0.00%\n--------------------------------------------------------------------------------\nTOTAL                                9,720,864       100.0%\n================================================================================\n\nModel Configuration:\n  d_model: 216\n  d_ff: 574\n  n_layers: 11\n  n_heads: 8\n  vocab_size: 16000\n  embedding_ratio: 35.00%\n  glu_expansion: 2.66x\n  tied_lm_head: True\n\nTarget params: 10,000,000\nActual params: 9,720,864\nDifference: -279,136 (-2.79%)\n================================================================================\n",
    "stderr": "05<00:01, 16.91it/s]\nEvaluating:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 93/118 [00:05<00:01, 16.25it/s]\nEvaluating:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 95/118 [00:05<00:01, 16.85it/s]\nEvaluating:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 98/118 [00:05<00:01, 18.98it/s]\nEvaluating:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 101/118 [00:05<00:00, 18.14it/s]\nEvaluating:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 103/118 [00:05<00:00, 15.39it/s]\nEvaluating:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 106/118 [00:05<00:00, 16.35it/s]\nEvaluating:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 108/118 [00:06<00:00, 16.44it/s]\nEvaluating:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 111/118 [00:06<00:00, 17.63it/s]\nEvaluating:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 113/118 [00:06<00:00, 15.65it/s]\nEvaluating:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 115/118 [00:06<00:00, 15.08it/s]\nEvaluating:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 117/118 [00:06<00:00, 14.41it/s]\n                                                             \n2025-10-28 20:34:54,138 - train - INFO - Final metrics: {'eval_loss': 0.052023429217490744, 'eval_perplexity': 1.0534004225718516}\n2025-10-28 20:34:54,337 - utils.checkpointing - INFO - Checkpoint saved to checkpoints/checkpoint_final.pt\n"
  },
  {
    "exp_name": "phase1_glu4x",
    "status": "success",
    "elapsed_seconds": 239.73656010627747,
    "stdout": "                0       0.00%\n  (tied={config.tied_lm_head})\n--------------------------------------------------------------------------------\nTransformer Backbone:                6,153,840      63.31%\n  Attention (per layer)                186,624\n  FFN/SwiGLU (per layer)               371,952\n  LayerNorm (per layer)                    864\n  Num layers                                11\nFinal LayerNorm                            432       0.00%\n--------------------------------------------------------------------------------\nTOTAL                                9,720,864       100.0%\n================================================================================\n\nModel Configuration:\n  d_model: 216\n  d_ff: 574\n  n_layers: 11\n  n_heads: 8\n  vocab_size: 16000\n  embedding_ratio: 35.00%\n  glu_expansion: 2.66x\n  tied_lm_head: True\n\nTarget params: 10,000,000\nActual params: 9,720,864\nDifference: -279,136 (-2.79%)\n================================================================================\n",
    "stderr": ":05<00:01, 16.89it/s]\nEvaluating:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 93/118 [00:05<00:01, 16.16it/s]\nEvaluating:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 95/118 [00:05<00:01, 16.82it/s]\nEvaluating:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 98/118 [00:05<00:01, 18.87it/s]\nEvaluating:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 101/118 [00:05<00:00, 18.17it/s]\nEvaluating:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 103/118 [00:05<00:00, 15.38it/s]\nEvaluating:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 106/118 [00:05<00:00, 16.26it/s]\nEvaluating:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 108/118 [00:06<00:00, 16.34it/s]\nEvaluating:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 111/118 [00:06<00:00, 17.51it/s]\nEvaluating:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 113/118 [00:06<00:00, 15.66it/s]\nEvaluating:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 115/118 [00:06<00:00, 15.10it/s]\nEvaluating:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 117/118 [00:06<00:00, 14.41it/s]\n                                                             \n2025-10-28 20:38:54,290 - train - INFO - Final metrics: {'eval_loss': 0.05326359134088171, 'eval_perplexity': 1.0547076202776913}\n2025-10-28 20:38:54,436 - utils.checkpointing - INFO - Checkpoint saved to checkpoints/checkpoint_final.pt\n"
  }
]