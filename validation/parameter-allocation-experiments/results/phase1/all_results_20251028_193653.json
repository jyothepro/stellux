[
  {
    "exp_name": "phase1_emb25",
    "status": "success",
    "elapsed_seconds": 237.56061029434204,
    "stdout": "                0       0.00%\n  (tied={config.tied_lm_head})\n--------------------------------------------------------------------------------\nTransformer Backbone:                6,153,840      63.31%\n  Attention (per layer)                186,624\n  FFN/SwiGLU (per layer)               371,952\n  LayerNorm (per layer)                    864\n  Num layers                                11\nFinal LayerNorm                            432       0.00%\n--------------------------------------------------------------------------------\nTOTAL                                9,720,864       100.0%\n================================================================================\n\nModel Configuration:\n  d_model: 216\n  d_ff: 574\n  n_layers: 11\n  n_heads: 8\n  vocab_size: 16000\n  embedding_ratio: 35.00%\n  glu_expansion: 2.66x\n  tied_lm_head: True\n\nTarget params: 10,000,000\nActual params: 9,720,864\nDifference: -279,136 (-2.79%)\n================================================================================\n",
    "stderr": "05<00:01, 16.96it/s]\nEvaluating:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 93/118 [00:05<00:01, 16.23it/s]\nEvaluating:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 95/118 [00:05<00:01, 16.84it/s]\nEvaluating:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 98/118 [00:05<00:01, 18.92it/s]\nEvaluating:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 101/118 [00:05<00:00, 18.23it/s]\nEvaluating:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 103/118 [00:05<00:00, 15.51it/s]\nEvaluating:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 106/118 [00:05<00:00, 16.46it/s]\nEvaluating:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 108/118 [00:06<00:00, 16.55it/s]\nEvaluating:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 111/118 [00:06<00:00, 17.71it/s]\nEvaluating:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 113/118 [00:06<00:00, 15.70it/s]\nEvaluating:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 115/118 [00:06<00:00, 15.18it/s]\nEvaluating:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 117/118 [00:06<00:00, 14.49it/s]\n                                                             \n2025-10-28 19:12:56,972 - train - INFO - Final metrics: {'eval_loss': 0.052549515070116265, 'eval_perplexity': 1.0539547474297644}\n2025-10-28 19:12:57,121 - utils.checkpointing - INFO - Checkpoint saved to checkpoints/checkpoint_final.pt\n"
  },
  {
    "exp_name": "phase1_emb35",
    "status": "success",
    "elapsed_seconds": 239.13337326049805,
    "stdout": "                0       0.00%\n  (tied={config.tied_lm_head})\n--------------------------------------------------------------------------------\nTransformer Backbone:                6,153,840      63.31%\n  Attention (per layer)                186,624\n  FFN/SwiGLU (per layer)               371,952\n  LayerNorm (per layer)                    864\n  Num layers                                11\nFinal LayerNorm                            432       0.00%\n--------------------------------------------------------------------------------\nTOTAL                                9,720,864       100.0%\n================================================================================\n\nModel Configuration:\n  d_model: 216\n  d_ff: 574\n  n_layers: 11\n  n_heads: 8\n  vocab_size: 16000\n  embedding_ratio: 35.00%\n  glu_expansion: 2.66x\n  tied_lm_head: True\n\nTarget params: 10,000,000\nActual params: 9,720,864\nDifference: -279,136 (-2.79%)\n================================================================================\n",
    "stderr": ":05<00:01, 16.99it/s]\nEvaluating:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 93/118 [00:05<00:01, 16.21it/s]\nEvaluating:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 95/118 [00:05<00:01, 16.70it/s]\nEvaluating:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 98/118 [00:05<00:01, 18.80it/s]\nEvaluating:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 101/118 [00:05<00:00, 18.03it/s]\nEvaluating:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 103/118 [00:05<00:00, 15.38it/s]\nEvaluating:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 106/118 [00:05<00:00, 16.30it/s]\nEvaluating:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 108/118 [00:06<00:00, 16.36it/s]\nEvaluating:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 111/118 [00:06<00:00, 17.63it/s]\nEvaluating:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 113/118 [00:06<00:00, 15.71it/s]\nEvaluating:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 115/118 [00:06<00:00, 15.13it/s]\nEvaluating:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 117/118 [00:06<00:00, 14.44it/s]\n                                                             \n2025-10-28 19:16:56,135 - train - INFO - Final metrics: {'eval_loss': 0.05273510657567927, 'eval_perplexity': 1.0541503706305757}\n2025-10-28 19:16:56,294 - utils.checkpointing - INFO - Checkpoint saved to checkpoints/checkpoint_final.pt\n"
  },
  {
    "exp_name": "phase1_emb45",
    "status": "success",
    "elapsed_seconds": 239.5720570087433,
    "stdout": "                0       0.00%\n  (tied={config.tied_lm_head})\n--------------------------------------------------------------------------------\nTransformer Backbone:                6,153,840      63.31%\n  Attention (per layer)                186,624\n  FFN/SwiGLU (per layer)               371,952\n  LayerNorm (per layer)                    864\n  Num layers                                11\nFinal LayerNorm                            432       0.00%\n--------------------------------------------------------------------------------\nTOTAL                                9,720,864       100.0%\n================================================================================\n\nModel Configuration:\n  d_model: 216\n  d_ff: 574\n  n_layers: 11\n  n_heads: 8\n  vocab_size: 16000\n  embedding_ratio: 35.00%\n  glu_expansion: 2.66x\n  tied_lm_head: True\n\nTarget params: 10,000,000\nActual params: 9,720,864\nDifference: -279,136 (-2.79%)\n================================================================================\n",
    "stderr": ":04<00:01, 16.86it/s]\nEvaluating:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 93/118 [00:05<00:01, 16.21it/s]\nEvaluating:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 95/118 [00:05<00:01, 16.83it/s]\nEvaluating:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 98/118 [00:05<00:01, 18.99it/s]\nEvaluating:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 101/118 [00:05<00:00, 18.15it/s]\nEvaluating:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 103/118 [00:05<00:00, 15.42it/s]\nEvaluating:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 106/118 [00:05<00:00, 16.33it/s]\nEvaluating:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 108/118 [00:06<00:00, 16.40it/s]\nEvaluating:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 111/118 [00:06<00:00, 17.61it/s]\nEvaluating:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 113/118 [00:06<00:00, 15.68it/s]\nEvaluating:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 115/118 [00:06<00:00, 15.15it/s]\nEvaluating:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 117/118 [00:06<00:00, 14.47it/s]\n                                                             \n2025-10-28 19:20:55,744 - train - INFO - Final metrics: {'eval_loss': 0.056202301755547523, 'eval_perplexity': 1.057811659247775}\n2025-10-28 19:20:55,894 - utils.checkpointing - INFO - Checkpoint saved to checkpoints/checkpoint_final.pt\n"
  },
  {
    "exp_name": "phase1_glu2x",
    "status": "success",
    "elapsed_seconds": 238.76552724838257,
    "stdout": "                0       0.00%\n  (tied={config.tied_lm_head})\n--------------------------------------------------------------------------------\nTransformer Backbone:                6,153,840      63.31%\n  Attention (per layer)                186,624\n  FFN/SwiGLU (per layer)               371,952\n  LayerNorm (per layer)                    864\n  Num layers                                11\nFinal LayerNorm                            432       0.00%\n--------------------------------------------------------------------------------\nTOTAL                                9,720,864       100.0%\n================================================================================\n\nModel Configuration:\n  d_model: 216\n  d_ff: 574\n  n_layers: 11\n  n_heads: 8\n  vocab_size: 16000\n  embedding_ratio: 35.00%\n  glu_expansion: 2.66x\n  tied_lm_head: True\n\nTarget params: 10,000,000\nActual params: 9,720,864\nDifference: -279,136 (-2.79%)\n================================================================================\n",
    "stderr": "05<00:01, 16.81it/s]\nEvaluating:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 93/118 [00:05<00:01, 16.12it/s]\nEvaluating:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 95/118 [00:05<00:01, 16.65it/s]\nEvaluating:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 98/118 [00:05<00:01, 18.67it/s]\nEvaluating:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 101/118 [00:05<00:00, 17.96it/s]\nEvaluating:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 103/118 [00:05<00:00, 15.29it/s]\nEvaluating:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 106/118 [00:05<00:00, 16.22it/s]\nEvaluating:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 108/118 [00:06<00:00, 16.33it/s]\nEvaluating:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 111/118 [00:06<00:00, 17.55it/s]\nEvaluating:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 113/118 [00:06<00:00, 15.61it/s]\nEvaluating:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 115/118 [00:06<00:00, 15.07it/s]\nEvaluating:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 117/118 [00:06<00:00, 14.42it/s]\n                                                             \n2025-10-28 19:24:54,430 - train - INFO - Final metrics: {'eval_loss': 0.053431485989626416, 'eval_perplexity': 1.0548847149093306}\n2025-10-28 19:24:54,608 - utils.checkpointing - INFO - Checkpoint saved to checkpoints/checkpoint_final.pt\n"
  },
  {
    "exp_name": "phase1_glu266x",
    "status": "success",
    "elapsed_seconds": 239.2858064174652,
    "stdout": "                0       0.00%\n  (tied={config.tied_lm_head})\n--------------------------------------------------------------------------------\nTransformer Backbone:                6,153,840      63.31%\n  Attention (per layer)                186,624\n  FFN/SwiGLU (per layer)               371,952\n  LayerNorm (per layer)                    864\n  Num layers                                11\nFinal LayerNorm                            432       0.00%\n--------------------------------------------------------------------------------\nTOTAL                                9,720,864       100.0%\n================================================================================\n\nModel Configuration:\n  d_model: 216\n  d_ff: 574\n  n_layers: 11\n  n_heads: 8\n  vocab_size: 16000\n  embedding_ratio: 35.00%\n  glu_expansion: 2.66x\n  tied_lm_head: True\n\nTarget params: 10,000,000\nActual params: 9,720,864\nDifference: -279,136 (-2.79%)\n================================================================================\n",
    "stderr": "04<00:01, 16.87it/s]\nEvaluating:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 93/118 [00:05<00:01, 16.20it/s]\nEvaluating:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 95/118 [00:05<00:01, 16.78it/s]\nEvaluating:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 98/118 [00:05<00:01, 18.92it/s]\nEvaluating:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 101/118 [00:05<00:00, 17.97it/s]\nEvaluating:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 103/118 [00:05<00:00, 15.25it/s]\nEvaluating:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 106/118 [00:05<00:00, 16.17it/s]\nEvaluating:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 108/118 [00:06<00:00, 16.27it/s]\nEvaluating:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 111/118 [00:06<00:00, 17.48it/s]\nEvaluating:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 113/118 [00:06<00:00, 15.66it/s]\nEvaluating:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 115/118 [00:06<00:00, 15.12it/s]\nEvaluating:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 117/118 [00:06<00:00, 14.45it/s]\n                                                             \n2025-10-28 19:28:53,762 - train - INFO - Final metrics: {'eval_loss': 0.053863517662629166, 'eval_perplexity': 1.0553405569795538}\n2025-10-28 19:28:53,923 - utils.checkpointing - INFO - Checkpoint saved to checkpoints/checkpoint_final.pt\n"
  },
  {
    "exp_name": "phase1_glu3x",
    "status": "success",
    "elapsed_seconds": 239.03076887130737,
    "stdout": "                0       0.00%\n  (tied={config.tied_lm_head})\n--------------------------------------------------------------------------------\nTransformer Backbone:                6,153,840      63.31%\n  Attention (per layer)                186,624\n  FFN/SwiGLU (per layer)               371,952\n  LayerNorm (per layer)                    864\n  Num layers                                11\nFinal LayerNorm                            432       0.00%\n--------------------------------------------------------------------------------\nTOTAL                                9,720,864       100.0%\n================================================================================\n\nModel Configuration:\n  d_model: 216\n  d_ff: 574\n  n_layers: 11\n  n_heads: 8\n  vocab_size: 16000\n  embedding_ratio: 35.00%\n  glu_expansion: 2.66x\n  tied_lm_head: True\n\nTarget params: 10,000,000\nActual params: 9,720,864\nDifference: -279,136 (-2.79%)\n================================================================================\n",
    "stderr": ":05<00:01, 16.76it/s]\nEvaluating:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 93/118 [00:05<00:01, 16.15it/s]\nEvaluating:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 95/118 [00:05<00:01, 16.60it/s]\nEvaluating:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 98/118 [00:05<00:01, 18.79it/s]\nEvaluating:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 101/118 [00:05<00:00, 18.04it/s]\nEvaluating:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 103/118 [00:05<00:00, 15.38it/s]\nEvaluating:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 106/118 [00:05<00:00, 16.33it/s]\nEvaluating:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 108/118 [00:06<00:00, 16.45it/s]\nEvaluating:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 111/118 [00:06<00:00, 17.68it/s]\nEvaluating:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 113/118 [00:06<00:00, 15.74it/s]\nEvaluating:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 115/118 [00:06<00:00, 15.21it/s]\nEvaluating:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 117/118 [00:06<00:00, 14.49it/s]\n                                                             \n2025-10-28 19:32:52,816 - train - INFO - Final metrics: {'eval_loss': 0.05236804157178453, 'eval_perplexity': 1.0537634999283663}\n2025-10-28 19:32:52,963 - utils.checkpointing - INFO - Checkpoint saved to checkpoints/checkpoint_final.pt\n"
  },
  {
    "exp_name": "phase1_glu4x",
    "status": "success",
    "elapsed_seconds": 239.36757493019104,
    "stdout": "                0       0.00%\n  (tied={config.tied_lm_head})\n--------------------------------------------------------------------------------\nTransformer Backbone:                6,153,840      63.31%\n  Attention (per layer)                186,624\n  FFN/SwiGLU (per layer)               371,952\n  LayerNorm (per layer)                    864\n  Num layers                                11\nFinal LayerNorm                            432       0.00%\n--------------------------------------------------------------------------------\nTOTAL                                9,720,864       100.0%\n================================================================================\n\nModel Configuration:\n  d_model: 216\n  d_ff: 574\n  n_layers: 11\n  n_heads: 8\n  vocab_size: 16000\n  embedding_ratio: 35.00%\n  glu_expansion: 2.66x\n  tied_lm_head: True\n\nTarget params: 10,000,000\nActual params: 9,720,864\nDifference: -279,136 (-2.79%)\n================================================================================\n",
    "stderr": "04<00:01, 16.92it/s]\nEvaluating:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 93/118 [00:05<00:01, 16.31it/s]\nEvaluating:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 95/118 [00:05<00:01, 16.77it/s]\nEvaluating:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 98/118 [00:05<00:01, 18.95it/s]\nEvaluating:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 101/118 [00:05<00:00, 18.13it/s]\nEvaluating:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 103/118 [00:05<00:00, 15.45it/s]\nEvaluating:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 106/118 [00:05<00:00, 16.33it/s]\nEvaluating:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 108/118 [00:06<00:00, 16.45it/s]\nEvaluating:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 111/118 [00:06<00:00, 17.70it/s]\nEvaluating:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 113/118 [00:06<00:00, 15.76it/s]\nEvaluating:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 115/118 [00:06<00:00, 15.21it/s]\nEvaluating:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 117/118 [00:06<00:00, 14.51it/s]\n                                                             \n2025-10-28 19:36:52,138 - train - INFO - Final metrics: {'eval_loss': 0.054223207455683264, 'eval_perplexity': 1.0557202204825453}\n2025-10-28 19:36:52,307 - utils.checkpointing - INFO - Checkpoint saved to checkpoints/checkpoint_final.pt\n"
  }
]